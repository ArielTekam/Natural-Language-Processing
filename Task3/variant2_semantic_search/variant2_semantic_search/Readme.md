```markdown
# Task 3 — Variant 2  
## Семантический поиск на основе векторных представлений

---

## Описание задания

В рамках **Task 3 (Variant 2)** реализована простая система **семантического поиска** по коллекции текстовых документов с использованием векторных представлений предложений.

Система позволяет:
- индексировать текстовые документы в векторном пространстве;
- выполнять поиск по смыслу (semantic search), а не по точному совпадению слов;
- получать результаты в текстовом и JSON-формате через CLI;
- масштабировать объём индексируемых данных.

---

## Используемые технологии

- **Python 3.12 (conda environment)**
- **Hugging Face sentence-transformers**
- **FAISS (CPU)** — векторное хранилище
- **NumPy**
- **Gzip (.txt.gz)** — формат входных данных

---

## Структура репозитория

```

variant2_semantic_search/
├── index.py              # Индексация документов
├── search.py             # Поиск по FAISS индексу
├── utils.py              # Вспомогательные функции (чтение .gz, нормализация)
├── requirements.txt      # Зависимости проекта
├── news.txt.gz           # Коллекция документов (gzip)
├── artifacts/            # Сгенерированные артефакты
│   ├── faiss.index
│   ├── meta.jsonl
│   └── config.json
└── screenshots/          # Скриншоты выполнения

````

---

## Исходные данные

В качестве коллекции документов используется архив **`news.txt.gz`**, содержащий текстовые новости на русском языке.  
Каждый документ обрабатывается и преобразуется в векторное представление с помощью модели sentence-transformers.

---

## Сравнение моделей

### 1. all-MiniLM-L6-v2

- Модель ориентирована в первую очередь на **английский язык**
- При поиске русскоязычных запросов результаты часто оказываются нерелевантными
- Пример: запрос *«экономика украины»* возвращает документы из категорий sport / culture

**Вывод:** модель не подходит для многоязычного или русскоязычного поиска

---

### 2. paraphrase-multilingual-MiniLM-L12-v2 (финальная)

- Поддерживает более 50 языков
- Корректно обрабатывает русские тексты
- Даёт существенно более высокую семантическую релевантность
- Улучшенные значения cosine similarity (≈ 0.6–0.75)

**Вывод:** оптимальный выбор для данного задания

---

## Обоснование выбора финальной модели

Модель **`paraphrase-multilingual-MiniLM-L12-v2`** была выбрана, так как:

- полностью соответствует языку коллекции (русский);
- не превышает ограничение по размеру модели;
- обеспечивает стабильное качество поиска;
- подходит для дальнейшего расширения (многоязычный поиск).

---

## Процесс выполнения

### 1. Индексация документов

```bash
python index.py --input news.txt.gz --out_dir artifacts --limit 2000 --normalize
````

В результате создаются:

* FAISS индекс (`faiss.index`)
* метаданные (`meta.jsonl`)
* конфигурация (`config.json`)

---

### 2. Поиск (текстовый вывод)

```bash
python search.py --query "экономика украины" --top_k 5 --show_text
```

---

### 3. Поиск (JSON-формат)

```bash
python search.py --query "экономика украины" --top_k 5 --json
```

---

## Метрика сходства

Используется **cosine similarity**, реализованная как **inner product на L2-нормализованных эмбеддингах**.
Значение score находится в диапазоне **[0, 1]**, где большее значение означает более высокую семантическую близость.

---

## Анализ результатов

После перехода на многоязычную модель:

* документы стали тематически соответствовать запросу;
* уменьшилось количество ложноположительных результатов;
* поиск стал устойчивым даже при увеличении объёма данных.

---

## Визуальные доказательства

В папке `screenshots/` представлены:

* настройка окружения и установка зависимостей;
* пример некорректных результатов с англоязычной моделью;
* успешная индексация;
* корректный семантический поиск (текст и JSON).

---

## Вывод

В ходе работы была реализована полноценная система семантического поиска с CLI-интерфейсом.
Проведено сравнение моделей, выявлены ограничения англоязычных эмбеддингов и обоснован выбор многоязычной модели.
Решение полностью соответствует требованиям задания и может быть расширено для больших коллекций документов.

```
```
